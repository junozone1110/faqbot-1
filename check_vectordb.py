#!/usr/bin/env python3
"""
ãƒ™ã‚¯ãƒˆãƒ«DBã®çŠ¶æ…‹ç¢ºèªã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import os
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from collections import Counter

load_dotenv()

CHROMA_DB_DIR = "./chroma_db_openai"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

def check_vectordb():
    """ãƒ™ã‚¯ãƒˆãƒ«DBã®çŠ¶æ…‹ã‚’ç¢ºèª"""
    # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    embedding_model = OpenAIEmbeddings(
        model="text-embedding-3-large",
        openai_api_key=OPENAI_API_KEY
    )
    
    # ãƒ™ã‚¯ãƒˆãƒ«DBã®èª­ã¿è¾¼ã¿
    vectordb = Chroma(
        persist_directory=CHROMA_DB_DIR,
        embedding_function=embedding_model
    )
    
    # å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
    collection = vectordb._collection
    all_docs = collection.get()
    
    print("=" * 80)
    print("ãƒ™ã‚¯ãƒˆãƒ«DB è©³ç´°æƒ…å ±")
    print("=" * 80)
    print()
    
    # åŸºæœ¬çµ±è¨ˆ
    total_docs = len(all_docs['ids'])
    print(f"ğŸ“Š ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {total_docs}")
    print()
    
    # ã‚½ãƒ¼ã‚¹åˆ¥ã®çµ±è¨ˆ
    if all_docs['metadatas']:
        sources = [meta.get('source', 'ä¸æ˜') for meta in all_docs['metadatas']]
        source_counts = Counter(sources)
        
        print("ğŸ“ ã‚½ãƒ¼ã‚¹åˆ¥ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°:")
        for source, count in sorted(source_counts.items(), key=lambda x: x[1], reverse=True):
            source_name = source.split('/')[-1] if '/' in source else source
            print(f"  â€¢ {source_name}: {count}ä»¶")
        print()
        
        # ãƒãƒ£ãƒ³ã‚¯IDã®ã‚µãƒ³ãƒ—ãƒ«
        chunk_ids = [meta.get('chunk_id', 'ä¸æ˜') for meta in all_docs['metadatas'][:10]]
        print("ğŸ”– ãƒãƒ£ãƒ³ã‚¯IDã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®10ä»¶ï¼‰:")
        for i, chunk_id in enumerate(chunk_ids, 1):
            print(f"  {i}. {chunk_id}")
        print()
    
    # ãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•çµ±è¨ˆ
    if all_docs['documents']:
        doc_lengths = [len(doc) for doc in all_docs['documents']]
        avg_length = sum(doc_lengths) / len(doc_lengths)
        min_length = min(doc_lengths)
        max_length = max(doc_lengths)
        
        print("ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ:")
        print(f"  â€¢ å¹³å‡é•·: {avg_length:.1f} æ–‡å­—")
        print(f"  â€¢ æœ€å°é•·: {min_length} æ–‡å­—")
        print(f"  â€¢ æœ€å¤§é•·: {max_length} æ–‡å­—")
        print()
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        print("ğŸ“„ ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆæœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ï¼‰:")
        sample_doc = all_docs['documents'][0]
        sample_meta = all_docs['metadatas'][0]
        print(f"  ã‚½ãƒ¼ã‚¹: {sample_meta.get('source', 'ä¸æ˜')}")
        print(f"  ãƒãƒ£ãƒ³ã‚¯ID: {sample_meta.get('chunk_id', 'ä¸æ˜')}")
        print(f"  å†…å®¹ï¼ˆæœ€åˆã®200æ–‡å­—ï¼‰:")
        print(f"    {sample_doc[:200]}...")
        print()
    
    print("=" * 80)
    print("âœ… ãƒ™ã‚¯ãƒˆãƒ«DBã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™")
    print("=" * 80)

if __name__ == "__main__":
    check_vectordb()


