#!/usr/bin/env python3
"""
FAQ Bot with Hybrid Search (OpenAI Embeddings + BM25)
ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’ä½¿ç”¨ã—ãŸFAQ Botã®ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ç‰ˆ
"""

import os
import argparse
from dotenv import load_dotenv

from langchain_openai import OpenAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

from hybrid_search import HybridSearchRetriever

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# å®šæ•°
CHROMA_DB_DIR = "./chroma_db_openai"
TOP_K_RESULTS = 5  # æ¤œç´¢çµæœã®ä¸Šä½ä»¶æ•°
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
PROMPT_TEMPLATE = """
ã‚ãªãŸã¯æ™¯å“è¡¨ç¤ºæ³•ã‚„FAQã«é–¢ã™ã‚‹è³ªå•ã«å›ç­”ã™ã‚‹è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’å‚è€ƒã«ã—ã¦ã€è³ªå•ã«æ—¥æœ¬èªã§åˆ†ã‹ã‚Šã‚„ã™ãå›ç­”ã—ã¦ãã ã•ã„ã€‚

**å›ç­”ã®æ§‹æˆ**:
ã¾ãšã€ŒğŸ“Œ è¦ç´„ã€ã¨ã—ã¦ã€æœ€ã‚‚é¡ä¼¼åº¦ãŒé«˜ã„å‚ç…§æ–‡æ›¸ï¼ˆ[å‚ç…§1]ã¾ãŸã¯[å‚ç…§2]ï¼‰ã‹ã‚‰ã€è³ªå•ã«ç›´æ¥é–¢é€£ã™ã‚‹éƒ¨åˆ†ã‚’æŠœç²‹ãƒ»è¦ç´„ã—ã¦ãã ã•ã„ã€‚
ã“ã®è¦ç´„ã§ã¯ã€æ–‡æ›¸ã®å†…å®¹ã‚’ãã®ã¾ã¾ä¼ãˆã€ç‹¬è‡ªã®è§£é‡ˆã‚„æ¨æ¸¬ã¯ä¸€åˆ‡åŠ ãˆãªã„ã§ãã ã•ã„ã€‚
ãã®å¾Œã€è©³ç´°ãªèª¬æ˜ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚

**é‡è¦ãªæŒ‡ç¤º**:
1. **è¦ç´„ã‚»ã‚¯ã‚·ãƒ§ãƒ³**:
   - å†’é ­ã«ã€ŒğŸ“Œ è¦ç´„ã€ã¨ã—ã¦ã€å‚ç…§æ–‡æ›¸ã‹ã‚‰ã®ç›´æ¥çš„ãªå†…å®¹ã®ã¿ã‚’è¨˜è¼‰
   - æ–‡æ›¸ã«æ›¸ã‹ã‚Œã¦ã„ãªã„ã“ã¨ã¯æ›¸ã‹ãªã„
   - ç‹¬è‡ªã®è§£é‡ˆã€æ¨æ¸¬ã€ä¸€èˆ¬åŒ–ã¯è¡Œã‚ãªã„
   - å¿…ãšå‡ºå…¸ï¼ˆ[å‚ç…§1]ãªã©ï¼‰ã‚’æ˜è¨˜

2. **è©³ç´°èª¬æ˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³**:
   - å„æ–‡ã‚„æ®µè½ã®æœ€å¾Œã«ã€ãã®æƒ…å ±ãŒã©ã®å‚ç…§ã‹ã‚‰æ¥ã¦ã„ã‚‹ã‹ã‚’ï¼ˆ[å‚ç…§1]ï¼‰ã®ã‚ˆã†ã«è¨˜è¼‰
   - è¤‡æ•°ã®å‚ç…§ã‹ã‚‰æƒ…å ±ã‚’å¾—ãŸå ´åˆã¯ã€ï¼ˆ[å‚ç…§1, 2]ï¼‰ã®ã‚ˆã†ã«è¨˜è¼‰
   - æ³•å¾‹æ¡æ–‡ã€æ–½è¡Œè¦å‰‡ã€FAQ ãã‚Œãã‚Œã®æƒ…å ±ã‚’åŒºåˆ¥ã—ã¦æ´»ç”¨

3. **æ›¸å¼**:
   - å…¨ãé–¢é€£ã™ã‚‹æƒ…å ±ãŒãªã„å ´åˆã¯ã€ã€Œæä¾›ã•ã‚ŒãŸæƒ…å ±ã«ã¯ã€ã“ã®è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€ã¨ç­”ãˆã‚‹

# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
{context}

# è³ªå•
{question}

# å›ç­”ï¼ˆå¿…ãšä¸Šè¨˜ã®æ§‹æˆã«å¾“ã£ã¦ãã ã•ã„ï¼‰
"""


def load_vectordb_with_hybrid_search():
    """ãƒ™ã‚¯ãƒˆãƒ«DBã‚’èª­ã¿è¾¼ã¿ã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢retrieverã‚’ä½œæˆ"""
    if not os.path.exists(CHROMA_DB_DIR):
        raise FileNotFoundError(
            f"ãƒ™ã‚¯ãƒˆãƒ«DB ({CHROMA_DB_DIR}) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n"
            f"ã¾ãš prepare_database_openai.py ã‚’å®Ÿè¡Œã—ã¦ãƒ™ã‚¯ãƒˆãƒ«DBã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
        )
    
    # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    embedding_model = OpenAIEmbeddings(
        model="text-embedding-3-large",
        openai_api_key=OPENAI_API_KEY
    )
    
    # ãƒ™ã‚¯ãƒˆãƒ«DBã®èª­ã¿è¾¼ã¿
    vectordb = Chroma(
        persist_directory=CHROMA_DB_DIR,
        embedding_function=embedding_model
    )
    
    # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢retrieverã®ä½œæˆ
    hybrid_retriever = HybridSearchRetriever(
        vectordb=vectordb,
        alpha=0.5  # BM25ã¨ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’åŒã˜é‡ã¿ã§
    )
    
    return hybrid_retriever


def format_docs(docs):
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¦ã€å‚ç…§ç•ªå·ã‚’ä»˜ä¸"""
    context_parts = []
    for i, doc in enumerate(docs, 1):
        source = doc.metadata.get('source', 'ä¸æ˜')
        chunk_id = doc.metadata.get('chunk_id', 'ä¸æ˜')
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’çŸ­ç¸®
        if 'Q&A' in source:
            source_type = "FAQ"
        elif 'æ–½è¡Œè¦å‰‡' in source:
            source_type = "æ–½è¡Œè¦å‰‡"
        elif 'ä¸å½“æ™¯å“é¡åŠã³ä¸å½“è¡¨ç¤ºé˜²æ­¢æ³•.pdf' in source:
            source_type = "æ™¯è¡¨æ³•"
        else:
            source_type = source
        
        context_parts.append(
            f"[å‚ç…§{i}] (å‡ºå…¸: {source_type}, {source}, ID: {chunk_id})\n{doc.page_content}\n"
        )
    
    return "\n".join(context_parts)


def ask_question(query: str, hybrid_retriever):
    """è³ªå•ã«å¯¾ã—ã¦å›ç­”ã‚’ç”Ÿæˆ"""
    # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã§ä¸Šä½TOP_K_RESULTSä»¶ã‚’å–å¾—
    docs_and_scores = hybrid_retriever.search(query, k=TOP_K_RESULTS)
    docs = [doc for doc, score in docs_and_scores]
    
    # LLMã®åˆæœŸåŒ–
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        google_api_key=GOOGLE_API_KEY,
        temperature=0.2,
    )
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆ
    prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)
    
    # RAGãƒã‚§ãƒ¼ãƒ³ã®æ§‹ç¯‰
    rag_chain = (
        {
            "context": lambda x: format_docs(docs),
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
        | StrOutputParser()
    )
    
    # å›ç­”ã®ç”Ÿæˆ
    answer = rag_chain.invoke(query)
    
    # å‚ç…§å…ƒæƒ…å ±ã®æ•´å½¢
    references = []
    for i, (doc, score) in enumerate(docs_and_scores, 1):
        source = doc.metadata.get('source', 'ä¸æ˜')
        chunk_id = doc.metadata.get('chunk_id', 'ä¸æ˜')
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’çŸ­ç¸®
        if 'Q&A' in source:
            source_label = "FAQ"
        elif 'æ–½è¡Œè¦å‰‡' in source:
            source_label = "æ–½è¡Œè¦å‰‡"
        elif 'ä¸å½“æ™¯å“é¡åŠã³ä¸å½“è¡¨ç¤ºé˜²æ­¢æ³•.pdf' in source:
            source_label = "â˜…æ™¯è¡¨æ³•â˜…"
        else:
            source_label = source
        
        hybrid_score = doc.metadata.get('hybrid_score', 0)
        bm25_score = doc.metadata.get('bm25_score', 0)
        vector_score = doc.metadata.get('vector_score', 0)
        
        references.append(
            f"[{i}] {source_label} - {source}\n"
            f"    (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚³ã‚¢: {hybrid_score:.4f}, "
            f"BM25: {bm25_score:.4f}, ãƒ™ã‚¯ãƒˆãƒ«: {vector_score:.4f})\n"
            f"    ãƒãƒ£ãƒ³ã‚¯ID: {chunk_id}"
        )
    
    return answer, references


def main():
    parser = argparse.ArgumentParser(description="FAQ Bot with Hybrid Search")
    parser.add_argument("question", nargs="?", help="è³ªå•å†…å®¹")
    parser.add_argument("--top-k", type=int, default=TOP_K_RESULTS, 
                        help=f"æ¤œç´¢çµæœã®ä¸Šä½ä»¶æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {TOP_K_RESULTS}ï¼‰")
    args = parser.parse_args()
    
    # ç’°å¢ƒå¤‰æ•°ã®ãƒã‚§ãƒƒã‚¯
    if not OPENAI_API_KEY or not GOOGLE_API_KEY:
        print("ã‚¨ãƒ©ãƒ¼: OPENAI_API_KEY ã¨ GOOGLE_API_KEY ã‚’ .env ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨­å®šã—ã¦ãã ã•ã„")
        return
    
    print("=" * 60)
    print("FAQãƒœãƒƒãƒˆ (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ç‰ˆ)")
    print("=" * 60)
    print()
    
    # è³ªå•ã®å–å¾—
    if args.question:
        query = args.question
    else:
        query = input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")
    
    print(f"\nè³ªå•: {query}\n")
    
    try:
        print("ãƒ™ã‚¯ãƒˆãƒ«DBã‚’èª­ã¿è¾¼ã¿ä¸­...")
        hybrid_retriever = load_vectordb_with_hybrid_search()
        
        print("é–¢é€£æƒ…å ±ã‚’æ¤œç´¢ä¸­ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ä½¿ç”¨ï¼‰...")
        print(f"  - ä¸Šä½{TOP_K_RESULTS}ä»¶ã‚’å–å¾—")
        print("å›ç­”ã‚’ç”Ÿæˆä¸­...\n")
        
        answer, references = ask_question(query, hybrid_retriever)
        
        print("=" * 60)
        print("å›ç­”:")
        print(answer)
        print("\n" + "=" * 60)
        print("å‚ç…§å…ƒ:")
        for ref in references:
            print(f"  {ref}")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

